model_name: prolog
dataset_name: bgl
device: cuda
data_dir: ./dataset/bgl/
output_dir: ./experimental_results/RQ1/random/bgl/
folder: bgl/
log_file: BGL_2k.log
sample_size: None
sample_log_file: None
parser_type: None
log_format: None
regex: []
keep_para: False
st: 0.3
depth: 3
max_child: 100
tau: 0.5
is_process: False
is_instance: False
train_file: train_fixed100_instances.pkl
test_file: test_fixed100_instances.pkl
window_type: sliding
session_level: None
window_size: 5
step_size: 1
train_size: 0.4
train_ratio: 1
valid_ratio: 0.1
test_ratio: 1
max_epoch: 200
n_epochs_stop: 10
n_warm_up_epoch: 10
batch_size: 32
lr: 0.01
is_logkey: False
random_sample: False
is_time: False
min_freq: 1
seq_len: 10
min_len: 10
max_len: 512
mask_ratio: 0.5
adaptive_window: False
deepsvdd_loss: False
deepsvdd_loss_test: False
scale: None
hidden: 256
layers: 4
attn_heads: 4
num_workers: 5
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.0
sample: sliding_window
history_size: 10
embeddings: embeddings.json
sequentials: True
quantitatives: True
semantics: True
parameters: False
input_size: 1
hidden_size: 128
num_layers: 2
embedding_dim: 50
accumulation_step: 1
optimizer: adam
lr_decay_ratio: 0.1
num_candidates: 10
log_freq: 100
resume_path: False
num_encoder_layers: 1
num_decoder_layers: 1
dim_model: 300
num_heads: 8
dim_feedforward: 2048
transformers_dropout: 0.1
model_dir: ./experimental_results/RQ1/random/bgl/prolog/
train_vocab: ./experimental_results/RQ1/random/bgl/train.pkl
vocab_path: ./experimental_results/RQ1/random/bgl/prolog_vocab.pkl
model_path: ./experimental_results/RQ1/random/bgl/prolog/prolog_5.pth
scale_path: ./experimental_results/RQ1/random/bgl/prolog/scale.pkl
